{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation - Lab 2\n",
    "\n",
    "**Neural Language Modeling**\n",
    "\n",
    "In this lab, we will train a character level language model over an input text. We will also evaluate and sample (generate) text using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and setup\n",
    "\n",
    "We will be using Python3 and the Neural Network framework: [Chainer](http://chainer.org/)\n",
    "\n",
    "To keep computation manageable, we will only use CPU(s) to train the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainer\n",
    "\n",
    "Try and work through the basic [chainer tutorial](http://docs.chainer.org/en/stable/tutorial/basic.html). You can skip *advanced* chainer concepts such as the [trainer class](http://docs.chainer.org/en/stable/tutorial/basic.html#trainer)\n",
    "\n",
    "Finally, have a look at the multi-layer feedforward neural network [example on MNIST dataset](http://docs.chainer.org/en/stable/tutorial/basic.html#example-multi-layer-perceptron-on-mnist). The code for ```class MLP(Chain)``` definition shows how we can add [layers](http://docs.chainer.org/en/stable/reference/links.html#learnable-connections) (linear in this example, other choices include LTSM, GRU) and [activation functions](http://docs.chainer.org/en/stable/reference/functions.html#activation-functions) (RELU in this example, other choices include sigmoid, tanh ) in Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character-level language model\n",
    "\n",
    "### Background material\n",
    "This lab is based on text and code from several online resources. It is highly recommended to check out the following:\n",
    "- A great explanation and visualization of recurrent neural networks: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [numpy based code to train a character-level language model](https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "- [Chainer implementation of the above source code](https://github.com/yusuketomoto/chainer-char-rnn)\n",
    "- [Chainer reference](http://docs.chainer.org/en/stable/reference/)\n",
    "- [Really cool explanation of LSTM (and GRU)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will be using a dataset created by Andrej Karpathy by combining all of Shakespeare's work: [**tinyshakespeare**](https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare) to train our model.\n",
    "\n",
    "The input file: ```tinyshakespeare.txt``` can be found in the ```data/``` directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"------------- Shakespeare -------------\")\n",
    "!head -n 20 data/tinyshakespeare.txt\n",
    "print(\"---------------------------------------\")\n",
    "print(\"\\n------------- Polish -------------\")\n",
    "!head -n 20 data/pierwsza.txt\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that text is **structured**, i.e., there are speaker tags, and new line characters after each dialog. We leave everything intact including the **punctuation**.\n",
    "\n",
    "As we will be **predicting** characters, let's explore the number of unique characters and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_data_for_debugging = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input_txt(fname):\n",
    "    # define a dictionary. Each unique character will be a key in the dictionary\n",
    "    # and the value will be its count\n",
    "    chars = {}\n",
    "    with open(fname, \"r\") as f:\n",
    "        # read entire file content into buffer\n",
    "        if prune_data_for_debugging:\n",
    "            data = f.read()[:100000]\n",
    "        else:\n",
    "            data = f.read()\n",
    "    # count occurrences of each character\n",
    "    for c in data:\n",
    "        if c not in chars:\n",
    "            chars[c] = 1\n",
    "        else:\n",
    "            chars[c] += 1\n",
    "    return data, chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the training file location\n",
    "# True : use English\n",
    "# False: use Polish\n",
    "\n",
    "if False:\n",
    "    train_fname = os.path.join(\"data\", \"tinyshakespeare.txt\")\n",
    "    data_postfix = \"shakespeare\"\n",
    "else:\n",
    "    train_fname = os.path.join(\"data\", \"pierwsza.txt\")\n",
    "    data_postfix = \"tadeusz\"\n",
    "    \n",
    "data, chars = read_input_txt(train_fname)\n",
    "vocab_size = len(chars.keys())\n",
    "data_size = len(data)\n",
    "\n",
    "# 90%-10% split for test and validation data\n",
    "train_data = data[: data_size * 90 // 100]\n",
    "dev_data = data[data_size * 90 // 100 :]\n",
    "train_size = len(train_data)\n",
    "dev_size = len(dev_data)\n",
    "\n",
    "print(\"\\n{0:^30}\".format(\"data stats\"))\n",
    "print(\"{0:^30}\".format(\"-\" * len(\"data stats\")))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"total characters\", data_size))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"total unique chars\", len(chars.keys())))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"train characters\", train_size))\n",
    "print(\"{0:>20s} | {1:10d}\".format(\"test characters\", dev_size))\n",
    "\n",
    "# printing top 5 characters based on frequency\n",
    "\n",
    "print(\"\\n{0:^30}\".format(\"top 5 characters in data\"))\n",
    "print(\"{0:^30}\".format(\"-\" * len(\"top 5 characters in data\")))\n",
    "print(\"{0:>20s} | {1:>10s}\".format(\"char\", \"freq\"))\n",
    "out_rows = [(k, v) for k, v in sorted(list(chars.items()), reverse=True, key=lambda t:t[1])[:5]]\n",
    "for k, v in out_rows:\n",
    "    print(\"{0:>20s} | {1:>10d}\".format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "\n",
    "We perform minimal pre-processing of the dataset. This involves, for the purposes of efficiency, mapping each character to a unique integer id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare an integer representation of the dataset\n",
    "data_char_to_ids = []\n",
    "for c in data:\n",
    "    data_char_to_ids.append(char_to_ix[c])\n",
    "print(\"****original text excerpt**** \\n{0:s}\".format(data[:50]))\n",
    "print(\"****text converted to integer representation**** \\n{0:s}\".format(\"\".join(map(str, \n",
    "                                                                       data_char_to_ids[:50]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Shakespeare\n",
    "The above table shows that the \" \" (space/blank) character occurs most often. As we train our language model, will the model learn proper word boundaries? That is, can the model predict when to insert a space between sequences?\n",
    "\n",
    "We will answer this question using empirical observations by sampling our model to generate text.\n",
    "\n",
    "[Spoiler - Shakespeare generated by Karpathy's model](http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss function\n",
    "\n",
    "Cross-entropy loss is used to measure the total loss during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN models\n",
    "\n",
    "We now create an RNN base class using LSTM.\n",
    "\n",
    "Chainer reference can be found at: http://docs.chainer.org/en/stable/reference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(Chain):\n",
    "    # constructor\n",
    "    # vocab_size: indicates the number of unique characters in the vocabulary\n",
    "    # n_units: size of the hidden layer\n",
    "    # gpu_id: if >=0, use GPU, else CPU\n",
    "    def __init__(self, vocab_size, n_units, gpu_id, use_LSTM=True):\n",
    "        # initialise Chainer base class\n",
    "        super(RNN, self).__init__()\n",
    "        #---------------------------------------------------------\n",
    "        # construct Neural Network\n",
    "        #---------------------------------------------------------\n",
    "        # add embedding layer\n",
    "        # http://docs.chainer.org/en/stable/reference/links.html#embedid\n",
    "        #---------------------------------------------------------\n",
    "        self.add_link(\"embed\", L.EmbedID(vocab_size, n_units))\n",
    "        #---------------------------------------------------------\n",
    "        # add LSTM or GRU layer\n",
    "        if use_LSTM:\n",
    "            # http://docs.chainer.org/en/stable/reference/links.html#lstm\n",
    "            self.add_link(\"L1\", L.LSTM(n_units, n_units))\n",
    "        else:\n",
    "            # http://docs.chainer.org/en/stable/reference/links.html#gru\n",
    "            self.add_link(\"L1\", L.StatefulGRU(n_units, n_units))\n",
    "        '''\n",
    "        ------------------------------------------------------------------\n",
    "        Q3 - ADD CODE (around) HERE\n",
    "        Go deep, add 1 more LSTM/GRU layer\n",
    "        ------------------------------------------------------------------\n",
    "        '''\n",
    "        #---------------------------------------------------------\n",
    "        # add output layer\n",
    "        # http://docs.chainer.org/en/stable/reference/links.html#linear\n",
    "        self.add_link(\"out\", L.Linear(n_units,vocab_size))\n",
    "        #---------------------------------------------------------\n",
    "        \n",
    "    def reset_state(self):\n",
    "        # reset LSTM state\n",
    "        # NOTE: the name field using during add_link call \n",
    "        # is used to refer to the layer\n",
    "        self.L1.reset_state()\n",
    "        \n",
    "    # function to compute the forward pass through the network layers\n",
    "    def forward(self, word):\n",
    "        # lookup character embedding and compute the hidden state\n",
    "        h1 = self.L1(self.embed(word))\n",
    "        # compute the output layer over the hidden state\n",
    "        out = self.out(h1)\n",
    "        return out\n",
    "    \n",
    "    # function to compute the loss for training\n",
    "    def __call__(self, c_n1, c_n2):\n",
    "        # call forward to predict output\n",
    "        # calculate softmax and then the cross entropy loss\n",
    "        # Chainer (and most NN frameworks) provide functions to compute \n",
    "        # the softmax and cross entropy loss together\n",
    "        self.loss = F.softmax_cross_entropy(self.forward(c_n1), c_n2)\n",
    "        # return loss\n",
    "        return self.loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU/GPU selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if >= 0, use GPU, if negative use CPU\n",
    "gpuid = -1\n",
    "\n",
    "# use cuda if GPU or numpy if CPU\n",
    "xp = cuda.cupy if gpuid >= 0 else np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_model(model):\n",
    "    #---------------------------------------------------------\n",
    "    # copy model to GPU if selected\n",
    "    #---------------------------------------------------------\n",
    "    if gpuid >= 0:\n",
    "        cuda.get_device(gpuid).use()\n",
    "        model.to_gpu()\n",
    "    #---------------------------------------------------------\n",
    "    # optimizer\n",
    "    # Select an optimizer\n",
    "    # http://docs.chainer.org/en/stable/reference/optimizers.html\n",
    "    # Alternatives: SGD, AdaGrad, RMSprop, etc\n",
    "    # We can also add weight decay\n",
    "    #---------------------------------------------------------\n",
    "    optimizer = optimizers.Adam()\n",
    "    # link optimizer to model\n",
    "    optimizer.setup(model)\n",
    "    # add weight decay\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(rate=0.0005))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# define model\n",
    "#---------------------------------------------------------\n",
    "use_LSTM = False\n",
    "lstm_postfix = \"lstm\" if use_LSTM else \"gru\"\n",
    "model_baseline = RNN(vocab_size, 50, gpuid, use_LSTM=use_LSTM)\n",
    "# the name field is used for logging stats and storing the model parameters\n",
    "model_baseline.__dict__['name'] = \"baseline_{0:s}_{1:s}\".format(data_postfix, lstm_postfix)\n",
    "'''\n",
    "------------------------------------------------------------------\n",
    "Q2 - ADD CODE (around) HERE\n",
    "\n",
    "A. Define a new model using GRU instead of LSTM\n",
    "B. Train and compute loss and perplexity as done for model_baseline\n",
    "C. Compare the performance and training time between the GRU and LSTMs\n",
    "------------------------------------------------------------------\n",
    "'''\n",
    "optimizer_baseline = setup_model(model_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train a batch of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(batch_data, model, optimizer):\n",
    "    # reset LSTM initial state before training each batch\n",
    "    model.reset_state()\n",
    "    \n",
    "    # reset loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for each character in the batch, the target character is the next in sequence\n",
    "    for i, (curr_char, next_char) in enumerate(zip(batch_data, batch_data[1:]), start=1):\n",
    "        # convert curr char into a chainer variable\n",
    "        c1 = Variable(xp.asarray([curr_char], dtype=np.int32), volatile=False)\n",
    "        c2 = Variable(xp.asarray([next_char], dtype=np.int32), volatile=False)\n",
    "        # compute loss for each character and the prediction\n",
    "        loss += model(c1, c2)\n",
    "    \n",
    "    # reset model gradients\n",
    "    model.cleargrads()\n",
    "    # compute loss through back prop\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optimizer.update()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(s_chars, n_chars, model):\n",
    "    # Sample n_chars, with starting char: s_char\n",
    "    sample_text = s_chars[:]\n",
    "    \n",
    "    model.reset_state()\n",
    "    \n",
    "    # initialize with s_char\n",
    "    for s_char in s_chars:\n",
    "        # compute the prediction from the starting character\n",
    "        c = Variable(xp.asarray([char_to_ix[s_char]], dtype=np.int32), volatile=True)\n",
    "        out = model.forward(c)\n",
    "\n",
    "    prob = F.softmax(out)\n",
    "    \n",
    "    # Sample remaining characters\n",
    "    for i in range(n_chars):\n",
    "        # compute probability distribution over the characters using softmax\n",
    "        prob = F.softmax(out)\n",
    "        \n",
    "        # if gpu, convert into numpy array in order to sample\n",
    "        if gpuid >= 0:\n",
    "            prob = cuda.to_cpu(prob.data)[0].astype(np.float64)\n",
    "        else:\n",
    "            prob = prob.data[0].astype(np.float64)\n",
    "        \n",
    "        # normalize probability\n",
    "        prob /= np.sum(prob)\n",
    "\n",
    "        # Sample next character from the predicted probability distribution\n",
    "        index = np.random.choice(range(len(prob)), p=prob)\n",
    "\n",
    "        # add sampled character to result\n",
    "        sample_text.append(ix_to_char[index])\n",
    "        \n",
    "        c = Variable(xp.asarray([index], dtype=np.int32), volatile=True)\n",
    "        # feed the character into the model\n",
    "        out = model.forward(c)\n",
    "        \n",
    "    # combine sampled text into a string\n",
    "    sampled_txt = ''.join(sample_text)\n",
    "    return sampled_txt, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, data, n_epochs=3, batch_size=128, logging=True):\n",
    "    if logging and model.__dict__['name']:\n",
    "        train_log_fname = \"{0:s}_train.log\".format(model.__dict__['name'])\n",
    "        train_log = open(train_log_fname, \"w\")\n",
    "        train_writer = csv.writer(train_log, lineterminator=\"\\n\")\n",
    "        train_writer.writerow([\"iter\", \"loss\"])\n",
    "    \n",
    "    # compute the number of batches in the data\n",
    "    data_size = len(data)\n",
    "    num_batches = data_size // batch_size\n",
    "    \n",
    "    # print reference text excerpt\n",
    "    print(\"Reference text:\\n{0:s}\".format(data[1000:1100]))\n",
    "    # sample text from the model\n",
    "    # this is on the untrained model and the output will be random characters\n",
    "    print(\"Sampled text:\\n{0:s}\".format(sample(data[1000], 100, model_baseline)))\n",
    "    \n",
    "    # start training epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # start progress bar for current epoch\n",
    "        sys.stderr.flush()\n",
    "        with tqdm(total=data_size) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            for i in range(0, data_size, batch_size):\n",
    "                # loop through the entire data in chunks of batch_size\n",
    "                # note: using the integer id representation of the text\n",
    "                batch_data = data_char_to_ids[i:i+batch_size]\n",
    "                # call batch training\n",
    "                loss = train_batch(batch_data, model, optimizer)\n",
    "                # extract value of loss from Chainer variable returned\n",
    "                loss = float(loss.data)\n",
    "                # compute number of characters trained on so far\n",
    "                it = (epoch * data_size) + i + batch_size\n",
    "                # write loss to file\n",
    "                train_writer.writerow([it, loss])\n",
    "                '''\n",
    "                ------------------------------------------------------------------\n",
    "                Q1 - ADD CODE (around) HERE\n",
    "                \n",
    "                A. Compute perplexity and add this information to the train \n",
    "                log file\n",
    "                B. Compute perplixity over validation data and create a new log file\n",
    "                ------------------------------------------------------------------\n",
    "                '''\n",
    "                pbar.set_description(\"epoch={0:d}, loss={1:.6f}\".format(epoch+1, loss))\n",
    "                pbar.update(batch_size)\n",
    "                # Sample 10 times\n",
    "        # sample at the end of each epoch\n",
    "        print(\"Sampling from starting char={0:s}, {1:d} characters\".format(data[1000], 100))\n",
    "        print(\"Sampled text:\\n{0:s}\".format(sample(data[1000], 100, model)))\n",
    "    \n",
    "    if train_log:\n",
    "        train_log.close()\n",
    "        print(\"train log file: {0:s}\".format(train_log_fname))\n",
    "    # Save model\n",
    "    if model.__dict__['name']:\n",
    "        serializers.save_npz(\"{0:s}.npz\".format(model.__dict__['name']), model)\n",
    "        print(\"model file: {0:s}\".format(model.__dict__['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "# training batch size\n",
    "# compute loss for upto batch_size number of characters\n",
    "#---------------------------------------------------------\n",
    "batch_size = 128\n",
    "#---------------------------------------------------------\n",
    "# epochs\n",
    "# to stop training\n",
    "#--------------------------------------------------------- \n",
    "n_epochs = 1\n",
    "#---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_loop(model_baseline, optimizer_baseline,\n",
    "#            train_data, n_epochs=n_epochs, \n",
    "#            batch_size=batch_size, logging=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model_fname = \"{0:s}.npz\".format(model_baseline.__dict__['name'])\n",
    "train_log_fname = \"{0:s}_train.log\".format(model_baseline.__dict__['name'])\n",
    "\n",
    "serializers.load_npz(model_fname, model_baseline)\n",
    "log_train = np.loadtxt(train_log_fname, delimiter=\",\", skiprows=True).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[2000:2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Sampled text:\\n{0:s}\".format(sample(list(data[2000:2020]), 100, model_baseline)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polish_samples = [\"Zosia ze wstążki kokardę zrobił\", \n",
    "                  \"bocian ma skrzydła biał\"]\n",
    "\n",
    "s_ix = 1\n",
    "model_sample_results = []\n",
    "\n",
    "for i, s in enumerate(polish_samples):\n",
    "    model_sample_results.append(sample(list(polish_samples[i]), 1, model_baseline))\n",
    "    print(\"\\n------------Sampled text-----------:\\n{0:s}\".format(model_sample_results[i][0]))\n",
    "\n",
    "k = 10\n",
    "\n",
    "top_k_ixs = np.argpartition(model_sample_results[s_ix][1], -k, axis=None)[-k:]\n",
    "top_k_probs = model_sample_results[s_ix][1][np.argpartition(model_sample_results[s_ix][1], \n",
    "                                                       -k, axis=None)[-k:]]\n",
    "\n",
    "print(\"{0:>5s} | {1:6s}\".format(\"char\", \"prob\"))\n",
    "print(\"\\n\".join([\"{0:>5s} | {1:>.5f}\".format(c, p) \n",
    "                for c, p in zip([ix_to_char[ix] for ix in top_k_ixs], top_k_probs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample(list(data[2000:2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(log_train[0], log_train[1])\n",
    "ax1.set_xlim(0, (n_epochs * train_size))\n",
    "ax1.set_xlabel(\"iteration\")\n",
    "ax1.set_ylabel(\"loss\", color=\"r\")\n",
    "for tlbl in ax1.get_yticklabels():\n",
    "    tlbl.set_color(\"r\")\n",
    "plt.legend(['training loss'], bbox_to_anchor=(1.48, 1.05), framealpha=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - evaluation using perplexity\n",
    "\n",
    "\n",
    "Implement **perplexity** metric calculation. Perplexity is the standard measure used to evaluate language modeling performance.\n",
    "\n",
    "Add the following code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pending\n",
    "\n",
    "Defining a test set and evaluation using perplexity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## possible questions (in work)\n",
    "\n",
    "- What happens if we increase the batch size (or the training sequence)?\n",
    "- Modify the network to include more layers. Go deep vs go wide.\n",
    "- Back-propagation through Time? Why?\n",
    "- GRU vs LSTM?\n",
    "- Can we modify the code to a word level neural network model? This can also be an assignment question.\n",
    "- Plot embeddings over the words, or characters\n",
    "- Character level vs Word level RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
